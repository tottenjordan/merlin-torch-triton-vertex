{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224fcc86-6e72-4839-91dd-7bca0459fd2e",
   "metadata": {},
   "source": [
    "# Session-based Recommendation with XLNET\n",
    "\n",
    "> following [this tutorial](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/examples/getting-started-session-based/02-session-based-XLNet-with-PyT.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86894ee2-692e-4b30-8a20-e2ae78e4bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import glob\n",
    "import torch \n",
    "\n",
    "from transformers4rec import torch as tr\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt, RecallAt\n",
    "from transformers4rec.torch.utils.examples_utils import wipe_memory\n",
    "\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189cc5b-baa3-46a4-ba3e-1de4e9703091",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "LOCATION = 'us-central1'\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'\n",
    "VERTEX_SA = 'jt-vertex-sa@hybrid-vertex.iam.gserviceaccount.com'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"LOCATION: {LOCATION}\")\n",
    "print(f\"REGION: {REGION}\")\n",
    "print(f\"VERTEX_SA: {VERTEX_SA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20b5b2-c732-40d2-8b96-df10a5998e6d",
   "metadata": {},
   "source": [
    "## Set the schema object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85fd923-65bf-4767-9648-3519ccab2aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DATA_DIR: workspace/data\n",
      "TRANSFORMED_WORKFLOW: workspace/data/processed_nvt\n",
      "OUTPUT_WORKFLOW_DIR: workspace/data/workflow_etl\n",
      "OUTPUT_DIR: workspace/data/sessions_by_day\n",
      "TRAIN_PATHS: workspace/data/sessions_by_day/1\n"
     ]
    }
   ],
   "source": [
    "REPO_WORKSPACE = 'workspace'\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "INPUT_DATA_DIR=f'{REPO_WORKSPACE}/{DATA_DIR}'\n",
    "TRANSFORMED_WORKFLOW=f'{INPUT_DATA_DIR}/processed_nvt'\n",
    "OUTPUT_WORKFLOW_DIR=f'{INPUT_DATA_DIR}/workflow_etl'\n",
    "OUTPUT_DIR=f'{INPUT_DATA_DIR}/sessions_by_day'\n",
    "TRAIN_PATHS=f'{OUTPUT_DIR}/1' #/train.parquet'\n",
    "\n",
    "print(f\"INPUT_DATA_DIR: {INPUT_DATA_DIR}\")\n",
    "print(f\"TRANSFORMED_WORKFLOW: {TRANSFORMED_WORKFLOW}\")\n",
    "print(f\"OUTPUT_WORKFLOW_DIR: {OUTPUT_WORKFLOW_DIR}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "print(f\"TRAIN_PATHS: {TRAIN_PATHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b21632-6d41-409d-a3b1-9ce9f2d256be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "# SCHEMA_PATH = os.environ.get(\"INPUT_SCHEMA_PATH\", \"/workspace/data/processed_nvt/schema.pbtxt\")\n",
    "SCHEMA_PATH = f'{TRANSFORMED_WORKFLOW}/schema.pbtxt'\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d800a536-5c99-4344-bb25-6f26165ff9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"session_id\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"session_id\"\n",
      "    max: 19867\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"categorical\"\n",
      "    extra_metadata {\n",
      "      type_url: \"type.googleapis.com/google.protobuf.Struct\"\n",
      "      value: \"\\n\\021\\n\\013num_buckets\\022\\002\\010\\000\\n\\033\\n\\016freq_threshold\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\025\\n\\010max_size\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\030\\n\\013start_index\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n5\\n\\010cat_path\\022)\\032\\'.//categories/unique.session_id.parquet\\nG\\n\\017embedding_sizes\\0224*2\\n\\030\\n\\013cardinality\\022\\t\\021\\000\\000\\000\\000\\000g\\323@\\n\\026\\n\\tdimension\\022\\t\\021\\000\\000\\000\\000\\000\\200y@\\n\\034\\n\\017dtype_item_size\\022\\t\\021\\000\\000\\000\\000\\000\\000P@\\n\\r\\n\\007is_list\\022\\002 \\000\\n\\017\\n\\tis_ragged\\022\\002 \\000\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"day-first\"\n",
      "  type: INT\n",
      "  annotation {\n"
     ]
    }
   ],
   "source": [
    "!head -20 $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a027fcc2-6502-476b-a944-f85362d482d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can select a subset of features for training\n",
    "schema = schema.select_by_name(['item_id-list', \n",
    "                                'category-list', \n",
    "                                'weekday_sin-list',\n",
    "                                'age_days-list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f150d5f4-f68b-4475-bfd4-23ed4105ac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace/data/processed_nvt/schema.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b72da6-2e07-4c5f-9b4a-d51f77f93df7",
   "metadata": {},
   "source": [
    "## Define the sequential input module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b4dd8d-a6c0-4936-83e2-abd72e13939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tr.TabularSequenceFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=20,\n",
    "        continuous_projection=64,\n",
    "        masking=\"mlm\",\n",
    "        d_output=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a5d8f-4501-4356-8af4-7ee7acf30506",
   "metadata": {},
   "source": [
    "## Define the Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f420ad-dba9-448e-b0f6-23e52f358f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XLNetConfig class and set default parameters for HF XLNet config  \n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",
    ")\n",
    "# Define the model block including: inputs, masking, projection and transformer block.\n",
    "body = tr.SequentialBlock(\n",
    "    inputs, tr.MLPBlock([64]), tr.TransformerBlock(transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "# Define the evaluation top-N metrics and the cut-offs\n",
    "metrics = [NDCGAt(top_ks=[20, 40], labels_onehot=True),  \n",
    "           RecallAt(top_ks=[20, 40], labels_onehot=True)]\n",
    "\n",
    "# Define a head related to next item prediction task \n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True, \n",
    "                              metrics=metrics),\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "# Get the end-to-end Model class \n",
    "model = tr.Model(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60768ecd-218d-45bc-bb19-ecf276385a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the end-to-end model \n",
    "model_test = transformer_config.to_torch_model(inputs, prediction_task)\n",
    "model_test\n",
    "# https://nvidia-merlin.github.io/Transformers4Rec/main/examples/end-to-end-session-based/02-End-to-end-session-based-with-Yoochoose-PyT.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93d4cb-e392-4d18-871b-cf6d503a4926",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151d90b-9a92-4a3b-88ce-dbb1bbd1b9f8",
   "metadata": {},
   "source": [
    "## set training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff77c07-6358-4e2c-8e32-98b6fe178425",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = int(os.environ.get(\n",
    "    \"per_device_train_batch_size\", \n",
    "    '128'\n",
    "))\n",
    "\n",
    "per_device_eval_batch_size = int(os.environ.get(\n",
    "    \"per_device_eval_batch_size\", \n",
    "    '32'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2cc360a-8b70-42e5-bbb6-7375f295624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec.config.trainer import T4RecTrainingArguments\n",
    "from transformers4rec.torch import Trainer\n",
    "# Set hyperparameters for training \n",
    "train_args = T4RecTrainingArguments(data_loader_engine='merlin', \n",
    "                                    dataloader_drop_last = True,\n",
    "                                    gradient_accumulation_steps = 1,\n",
    "                                    per_device_train_batch_size = per_device_train_batch_size, \n",
    "                                    per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "                                    output_dir = \"./tmp\", \n",
    "                                    learning_rate=0.0005,\n",
    "                                    lr_scheduler_type='cosine', \n",
    "                                    learning_rate_num_cosine_cycles_by_epoch=1.5,\n",
    "                                    num_train_epochs=5,\n",
    "                                    max_sequence_length=20, \n",
    "                                    report_to = [],\n",
    "                                    logging_steps=50,\n",
    "                                    no_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab44d1-90db-4b80-b824-aa977abde71e",
   "metadata": {},
   "source": [
    "## Daily Fine-Tuning: Training over a time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed2e6665-34a1-44bb-baf3-5c614cdfe847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the T4Rec Trainer, which manages training and evaluation for the PyTorch API\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ecae5e-6dc3-45a5-bec3-26cbb70a71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/workspace/data\")\n",
    "# OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", f\"{INPUT_DATA_DIR}/sessions_by_day\")\n",
    "\n",
    "start_window_index = int(os.environ.get(\n",
    "    \"start_window_index\", \n",
    "    '1'\n",
    "))\n",
    "\n",
    "final_window_index = int(os.environ.get(\n",
    "    \"final_window_index\", \n",
    "    '8'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f19290-738f-4e42-8461-412e54d61609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['workspace/data/sessions_by_day/1/train.parquet']\n",
      "********************\n",
      "Launch training for day 1 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1536\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.850800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 2 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 5.17643404006958\n",
      " eval_/next-item/ndcg_at_20 = 0.17471647262573242\n",
      " eval_/next-item/ndcg_at_40 = 0.22482089698314667\n",
      " eval_/next-item/recall_at_20 = 0.4322916865348816\n",
      " eval_/next-item/recall_at_40 = 0.6822916865348816\n",
      " eval_runtime = 0.1466\n",
      " eval_samples_per_second = 1309.324\n",
      " eval_steps_per_second = 40.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1792\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['workspace/data/sessions_by_day/2/train.parquet']\n",
      "********************\n",
      "Launch training for day 2 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.933700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "********************\n",
      "Eval results for day 3 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 4.593666076660156\n",
      " eval_/next-item/ndcg_at_20 = 0.21282625198364258\n",
      " eval_/next-item/ndcg_at_40 = 0.2544132471084595\n",
      " eval_/next-item/recall_at_20 = 0.53125\n",
      " eval_/next-item/recall_at_40 = 0.734375\n",
      " eval_runtime = 0.1513\n",
      " eval_samples_per_second = 1269.03\n",
      " eval_steps_per_second = 39.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1664\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['workspace/data/sessions_by_day/3/train.parquet']\n",
      "********************\n",
      "Launch training for day 3 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 00:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.585100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "********************\n",
      "Eval results for day 4 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 4.511531352996826\n",
      " eval_/next-item/ndcg_at_20 = 0.16087283194065094\n",
      " eval_/next-item/ndcg_at_40 = 0.23196697235107422\n",
      " eval_/next-item/recall_at_20 = 0.4322916865348816\n",
      " eval_/next-item/recall_at_40 = 0.7760416865348816\n",
      " eval_runtime = 0.1432\n",
      " eval_samples_per_second = 1340.964\n",
      " eval_steps_per_second = 41.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1536\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['workspace/data/sessions_by_day/4/train.parquet']\n",
      "********************\n",
      "Launch training for day 4 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.513200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "********************\n",
      "Eval results for day 5 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 4.298824787139893\n",
      " eval_/next-item/ndcg_at_20 = 0.22139111161231995\n",
      " eval_/next-item/ndcg_at_40 = 0.2713688910007477\n",
      " eval_/next-item/recall_at_20 = 0.5520833730697632\n",
      " eval_/next-item/recall_at_40 = 0.796875\n",
      " eval_runtime = 0.1409\n",
      " eval_samples_per_second = 1362.796\n",
      " eval_steps_per_second = 42.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1664\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['workspace/data/sessions_by_day/5/train.parquet']\n",
      "********************\n",
      "Launch training for day 5 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 00:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.495400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "********************\n",
      "Eval results for day 6 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 4.3619303703308105\n",
      " eval_/next-item/ndcg_at_20 = 0.18418318033218384\n",
      " eval_/next-item/ndcg_at_40 = 0.2430708408355713\n",
      " eval_/next-item/recall_at_20 = 0.5104166865348816\n",
      " eval_/next-item/recall_at_40 = 0.796875\n",
      " eval_runtime = 0.1489\n",
      " eval_samples_per_second = 1289.264\n",
      " eval_steps_per_second = 40.289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1664\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['workspace/data/sessions_by_day/6/train.parquet']\n",
      "********************\n",
      "Launch training for day 6 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 00:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.493300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "********************\n",
      "Eval results for day 7 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 4.410634517669678\n",
      " eval_/next-item/ndcg_at_20 = 0.2138700783252716\n",
      " eval_/next-item/ndcg_at_40 = 0.2573198974132538\n",
      " eval_/next-item/recall_at_20 = 0.5572916865348816\n",
      " eval_/next-item/recall_at_40 = 0.7708333730697632\n",
      " eval_runtime = 0.1427\n",
      " eval_samples_per_second = 1345.179\n",
      " eval_steps_per_second = 42.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1664\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['workspace/data/sessions_by_day/7/train.parquet']\n",
      "********************\n",
      "Launch training for day 7 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 00:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.482700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "********************\n",
      "Eval results for day 8 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 4.469639301300049\n",
      " eval_/next-item/ndcg_at_20 = 0.1843894124031067\n",
      " eval_/next-item/ndcg_at_40 = 0.23753295838832855\n",
      " eval_/next-item/recall_at_20 = 0.5052083730697632\n",
      " eval_/next-item/recall_at_40 = 0.765625\n",
      " eval_runtime = 0.1455\n",
      " eval_samples_per_second = 1319.515\n",
      " eval_steps_per_second = 41.235\n"
     ]
    }
   ],
   "source": [
    "start_time_window_index = start_window_index\n",
    "final_time_window_index = final_window_index\n",
    "#Iterating over days of one week\n",
    "for time_index in range(start_time_window_index, final_time_window_index):\n",
    "    # Set data \n",
    "    time_index_train = time_index\n",
    "    time_index_eval = time_index + 1\n",
    "    train_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\"))\n",
    "    eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"))\n",
    "    print(train_paths)\n",
    "    \n",
    "    # Train on day related to time_index \n",
    "    print('*'*20)\n",
    "    print(\"Launch training for day %s are:\" %time_index)\n",
    "    print('*'*20 + '\\n')\n",
    "    trainer.train_dataset_or_path = train_paths\n",
    "    trainer.reset_lr_scheduler()\n",
    "    trainer.train()\n",
    "    trainer.state.global_step +=1\n",
    "    print('finished')\n",
    "    \n",
    "    # Evaluate on the following day\n",
    "    trainer.eval_dataset_or_path = eval_paths\n",
    "    train_metrics = trainer.evaluate(metric_key_prefix='eval')\n",
    "    print('*'*20)\n",
    "    print(\"Eval results for day %s are:\\t\" %time_index_eval)\n",
    "    print('\\n' + '*'*20 + '\\n')\n",
    "    for key in sorted(train_metrics.keys()):\n",
    "        print(\" %s = %s\" % (key, str(train_metrics[key]))) \n",
    "    wipe_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c1f401-d007-4624-a8e5-ce82a9657be8",
   "metadata": {},
   "source": [
    "# Re-compute evaluation metrics of the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41d4a2b5-5afc-4b5f-8c33-25af295ad0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_data_paths: ['workspace/data/sessions_by_day/8/valid.parquet']\n"
     ]
    }
   ],
   "source": [
    "eval_data_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"))\n",
    "\n",
    "# eval_data_paths = f'{OUTPUT_DIR}/{time_index_eval}/valid.parquet'\n",
    "print(f\"eval_data_paths: {eval_data_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3952b20-847d-4243-ab7f-b9f6299cc854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval_/loss = 4.469639301300049\n",
      "  eval_/next-item/ndcg_at_20 = 0.1843894124031067\n",
      "  eval_/next-item/ndcg_at_40 = 0.23753295838832855\n",
      "  eval_/next-item/recall_at_20 = 0.5052083730697632\n",
      "  eval_/next-item/recall_at_40 = 0.765625\n",
      "  eval_runtime = 0.1484\n",
      "  eval_samples_per_second = 1294.091\n",
      "  eval_steps_per_second = 40.44\n"
     ]
    }
   ],
   "source": [
    "# set new data from day 7\n",
    "eval_metrics = trainer.evaluate(eval_dataset=eval_data_paths, metric_key_prefix='eval')\n",
    "for key in sorted(eval_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(eval_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249aa26-e4cd-4581-9364-91a2e315ff24",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fe14007-14be-41a1-8bf5-9b2083da7898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path= os.environ.get(\"OUTPUT_DIR\", f\"{INPUT_DATA_DIR}/saved_model\")\n",
    "MODEL_PATH = f'{INPUT_DATA_DIR}/saved_model'\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e594d1e8-94d9-4a37-9137-3c6e07ab527d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'workspace/data/workflow_etl'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_WORKFLOW_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9091b4a7-4744-405b-a246-0f565801878a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_parameter = next(model.parameters())\n",
    "input_shape = first_parameter.size()\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f1def1b-6557-455c-810b-1cca526aff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7955a0-b97b-4618-ae56-7e8ea60fd3e7",
   "metadata": {},
   "source": [
    "### Save method 2\n",
    "* see [here](https://nvidia-merlin.github.io/Transformers4Rec/main/examples/end-to-end-session-based/02-End-to-end-session-based-with-Yoochoose-PyT.html) for more details\n",
    "\n",
    "per Ronnay, don't use `export_pytorch_ensemble`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1033a3d-de0d-4c1a-bc56-80c0a26f25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow.load(OUTPUT_WORKFLOW_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cff8340-9f2a-4c98-afdf-67bccbfdf4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nvtabular.inference.triton import export_pytorch_ensemble\n",
    "\n",
    "# export_pytorch_ensemble(\n",
    "#     model,\n",
    "#     workflow,\n",
    "#     sparse_max=trainer.get_train_dataloader().dataset.sparse_max,\n",
    "#     name= \"t4r_pytorch\",\n",
    "#     model_path= \"./torch_ensemble/models/\",\n",
    "#     label_columns =[],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84564e08-0b1d-4b48-a486-f2f0ed797644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./workspace_v2/data/models\u001b[00m\n",
      "├── \u001b[01;34mt4r_pytorch\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34mt4r_pytorch_nvt\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   ├── model.py\n",
      "│   │   └── \u001b[01;34mworkflow\u001b[00m\n",
      "│   │       ├── \u001b[01;34mcategories\u001b[00m\n",
      "│   │       │   ├── unique.category.parquet\n",
      "│   │       │   ├── unique.item_id.parquet\n",
      "│   │       │   └── unique.session_id.parquet\n",
      "│   │       ├── metadata.json\n",
      "│   │       └── workflow.pkl\n",
      "│   └── config.pbtxt\n",
      "└── \u001b[01;34mt4r_pytorch_pt\u001b[00m\n",
      "    ├── \u001b[01;34m1\u001b[00m\n",
      "    │   ├── model.pkl\n",
      "    │   ├── model.pth\n",
      "    │   ├── model.py\n",
      "    │   └── model_info.json\n",
      "    └── config.pbtxt\n",
      "\n",
      "8 directories, 13 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./workspace_v2/data/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "037cf89e-6a1e-4395-b921-62fb47f967d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./workspace/data/models\u001b[00m\n",
      "├── \u001b[01;34m0_predictpytorchtriton\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.pt\n",
      "│   └── config.pbtxt\n",
      "└── \u001b[01;34mensemble_model\u001b[00m\n",
      "    ├── \u001b[01;34m1\u001b[00m\n",
      "    └── config.pbtxt\n",
      "\n",
      "4 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./workspace/data/models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
